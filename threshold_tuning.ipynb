{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Class Threshold Tuning for News Topic Classification\n",
    "\n",
    "Optimizes per-label decision thresholds for `ContextNews/news-classifier` using the validation split of `ContextNews/labelled_articles`.\n",
    "\n",
    "**Make sure to set Runtime > Change runtime type > T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate scikit-learn torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Model + Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"ContextNews/news-classifier\"\n",
    "DATASET_ID = \"ContextNews/labelled_articles\"\n",
    "\n",
    "TOPICS = [\n",
    "    \"politics\", \"geopolitics\", \"conflict\", \"crime\", \"law\", \"business\",\n",
    "    \"economy\", \"markets\", \"technology\", \"science\", \"health\", \"environment\",\n",
    "    \"society\", \"education\", \"sports\", \"entertainment\",\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model labels: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = load_dataset(DATASET_ID, split=\"validation\")\n",
    "print(f\"Validation set: {len(val_ds)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Inference on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_text(row):\n",
    "    title = row.get(\"title\") or \"\"\n",
    "    summary = row.get(\"summary\") or \"\"\n",
    "    text = row.get(\"text\") or \"\"\n",
    "    text_excerpt = \" \".join(text.split()[:300])\n",
    "    return \" \".join(p for p in [title, summary, text_excerpt] if p)\n",
    "\n",
    "\n",
    "def preprocess(row):\n",
    "    row[\"input_text\"] = build_input_text(row)\n",
    "    row[\"labels\"] = [float(row[t] or 0) for t in TOPICS]\n",
    "    return row\n",
    "\n",
    "\n",
    "val_ds = val_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "for i in range(0, len(val_ds), BATCH_SIZE):\n",
    "    batch = val_ds[i : i + BATCH_SIZE]\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    all_probs.append(probs)\n",
    "    all_labels.append(np.array(batch[\"labels\"]))\n",
    "\n",
    "    if (i // BATCH_SIZE) % 10 == 0:\n",
    "        print(f\"\\r  Processed {min(i + BATCH_SIZE, len(val_ds))}/{len(val_ds)}\", end=\"\", flush=True)\n",
    "\n",
    "print()\n",
    "\n",
    "y_probs = np.concatenate(all_probs, axis=0)\n",
    "y_true = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(f\"y_true shape:  {y_true.shape}\")\n",
    "print(f\"y_probs shape: {y_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Metrics (threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = (y_probs >= 0.5).astype(int)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"f1_micro\": f1_score(y_true, y_pred_baseline, average=\"micro\", zero_division=0),\n",
    "    \"f1_macro\": f1_score(y_true, y_pred_baseline, average=\"macro\", zero_division=0),\n",
    "    \"precision\": precision_score(y_true, y_pred_baseline, average=\"micro\", zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_pred_baseline, average=\"micro\", zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"Baseline metrics (threshold = 0.5):\")\n",
    "for k, v in baseline_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class F1 (baseline):\")\n",
    "baseline_per_class = {}\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    f1 = f1_score(y_true[:, i], y_pred_baseline[:, i], zero_division=0)\n",
    "    baseline_per_class[topic] = f1\n",
    "    print(f\"  {topic:15s} {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-Class Threshold Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_range = np.arange(0.05, 0.96, 0.01)\n",
    "\n",
    "optimal_thresholds = {}\n",
    "f1_curves = {}  # topic -> list of (threshold, f1)\n",
    "\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    best_t = 0.5\n",
    "    best_f1 = 0.0\n",
    "    curve = []\n",
    "\n",
    "    for t in thresholds_range:\n",
    "        preds = (y_probs[:, i] >= t).astype(int)\n",
    "        f1 = f1_score(y_true[:, i], preds, zero_division=0)\n",
    "        curve.append((t, f1))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = float(round(t, 2))\n",
    "\n",
    "    optimal_thresholds[topic] = best_t\n",
    "    f1_curves[topic] = curve\n",
    "    print(f\"  {topic:15s}  threshold={best_t:.2f}  F1={best_f1:.4f}  (baseline F1={baseline_per_class[topic]:.4f})\")\n",
    "\n",
    "print(\"\\nOptimal thresholds:\")\n",
    "print(json.dumps(optimal_thresholds, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate with Tuned Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = np.zeros_like(y_probs, dtype=int)\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    y_pred_tuned[:, i] = (y_probs[:, i] >= optimal_thresholds[topic]).astype(int)\n",
    "\n",
    "tuned_metrics = {\n",
    "    \"f1_micro\": f1_score(y_true, y_pred_tuned, average=\"micro\", zero_division=0),\n",
    "    \"f1_macro\": f1_score(y_true, y_pred_tuned, average=\"macro\", zero_division=0),\n",
    "    \"precision\": precision_score(y_true, y_pred_tuned, average=\"micro\", zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_pred_tuned, average=\"micro\", zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"Tuned metrics:\")\n",
    "for k, v in tuned_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(f\"  Baseline macro F1: {baseline_metrics['f1_macro']:.4f}\")\n",
    "print(f\"  Tuned macro F1:    {tuned_metrics['f1_macro']:.4f}\")\n",
    "improvement = tuned_metrics[\"f1_macro\"] - baseline_metrics[\"f1_macro\"]\n",
    "print(f\"  Improvement:       {improvement:+.4f}\")\n",
    "print(\"=\" * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Thresholds to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"news_classifier_thresholds.json\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(optimal_thresholds, f, indent=2)\n",
    "\n",
    "print(f\"Thresholds saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. F1 vs Threshold Curves (3 Worst Performing Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_f1 = sorted(baseline_per_class.items(), key=lambda x: x[1])\n",
    "worst_3 = [topic for topic, _ in sorted_by_f1[:3]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, topic in zip(axes, worst_3):\n",
    "    curve = f1_curves[topic]\n",
    "    ts = [t for t, _ in curve]\n",
    "    f1s = [f for _, f in curve]\n",
    "    best_t = optimal_thresholds[topic]\n",
    "    best_f1 = max(f1s)\n",
    "\n",
    "    ax.plot(ts, f1s, linewidth=1.5)\n",
    "    ax.axvline(x=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"default (0.5)\")\n",
    "    ax.axvline(x=best_t, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"optimal ({best_t:.2f})\")\n",
    "    ax.scatter([best_t], [best_f1], color=\"red\", zorder=5)\n",
    "    ax.set_title(topic)\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"F1\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"F1 vs Threshold - 3 Worst Performing Labels\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
