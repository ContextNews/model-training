{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Class Threshold Tuning for News Topic Classification\n",
    "\n",
    "Optimizes per-label decision thresholds for `ContextNews/news-classifier`.\n",
    "\n",
    "- **Tune** thresholds on the validation split\n",
    "- **Evaluate** with frozen thresholds on the held-out test split\n",
    "\n",
    "**Make sure to set Runtime > Change runtime type > T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate scikit-learn torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Model + Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"ContextNews/news-classifier\"\n",
    "DATASET_ID = \"ContextNews/labelled_articles\"\n",
    "\n",
    "TOPICS = [\n",
    "    \"politics\", \"geopolitics\", \"conflict\", \"crime\", \"law\", \"business\",\n",
    "    \"economy\", \"markets\", \"technology\", \"science\", \"health\", \"environment\",\n",
    "    \"society\", \"education\", \"sports\", \"entertainment\",\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model labels: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = load_dataset(DATASET_ID, split=\"validation\")\n",
    "test_ds = load_dataset(DATASET_ID, split=\"test\")\n",
    "print(f\"Validation set: {len(val_ds)} rows\")\n",
    "print(f\"Test set:       {len(test_ds)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Inference Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_text(row):\n",
    "    title = row.get(\"title\") or \"\"\n",
    "    summary = row.get(\"summary\") or \"\"\n",
    "    text = row.get(\"text\") or \"\"\n",
    "    text_excerpt = \" \".join(text.split()[:300])\n",
    "    return \" \".join(p for p in [title, summary, text_excerpt] if p)\n",
    "\n",
    "\n",
    "def preprocess(row):\n",
    "    row[\"input_text\"] = build_input_text(row)\n",
    "    row[\"labels\"] = [float(row[t] or 0) for t in TOPICS]\n",
    "    return row\n",
    "\n",
    "\n",
    "def run_inference(ds, batch_size=32):\n",
    "    \"\"\"Run batched inference, return (y_true, y_probs) numpy arrays.\"\"\"\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        batch = ds[i : i + batch_size]\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            batch[\"input_text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**encodings).logits\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(np.array(batch[\"labels\"]))\n",
    "\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"\\r  Processed {min(i + batch_size, len(ds))}/{len(ds)}\", end=\"\", flush=True)\n",
    "\n",
    "    print()\n",
    "    return np.concatenate(all_labels, axis=0), np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Inference on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = val_ds.map(preprocess)\n",
    "val_true, val_probs = run_inference(val_ds)\n",
    "print(f\"val_true shape:  {val_true.shape}\")\n",
    "print(f\"val_probs shape: {val_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Metrics on Validation (threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_baseline = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "val_baseline_metrics = {\n",
    "    \"f1_micro\": f1_score(val_true, val_pred_baseline, average=\"micro\", zero_division=0),\n",
    "    \"f1_macro\": f1_score(val_true, val_pred_baseline, average=\"macro\", zero_division=0),\n",
    "    \"precision\": precision_score(val_true, val_pred_baseline, average=\"micro\", zero_division=0),\n",
    "    \"recall\": recall_score(val_true, val_pred_baseline, average=\"micro\", zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"Validation baseline metrics (threshold = 0.5):\")\n",
    "for k, v in val_baseline_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class F1 (validation baseline):\")\n",
    "val_baseline_per_class = {}\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    f1 = f1_score(val_true[:, i], val_pred_baseline[:, i], zero_division=0)\n",
    "    val_baseline_per_class[topic] = f1\n",
    "    print(f\"  {topic:15s} {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Class Threshold Sweep (on Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_range = np.arange(0.05, 0.96, 0.01)\n",
    "\n",
    "optimal_thresholds = {}\n",
    "f1_curves = {}  # topic -> list of (threshold, f1)\n",
    "\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    best_t = 0.5\n",
    "    best_f1 = 0.0\n",
    "    curve = []\n",
    "\n",
    "    for t in thresholds_range:\n",
    "        preds = (val_probs[:, i] >= t).astype(int)\n",
    "        f1 = f1_score(val_true[:, i], preds, zero_division=0)\n",
    "        curve.append((t, f1))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = float(round(t, 2))\n",
    "\n",
    "    optimal_thresholds[topic] = best_t\n",
    "    f1_curves[topic] = curve\n",
    "    print(f\"  {topic:15s}  threshold={best_t:.2f}  F1={best_f1:.4f}  (baseline F1={val_baseline_per_class[topic]:.4f})\")\n",
    "\n",
    "print(\"\\nOptimal thresholds:\")\n",
    "print(json.dumps(optimal_thresholds, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Held-Out Test Set\n",
    "\n",
    "Thresholds were tuned on validation. To get an unbiased estimate, we freeze them and evaluate on the **test** split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.map(preprocess)\n",
    "test_true, test_probs = run_inference(test_ds)\n",
    "print(f\"test_true shape:  {test_true.shape}\")\n",
    "print(f\"test_probs shape: {test_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline on test (threshold = 0.5)\n",
    "test_pred_baseline = (test_probs >= 0.5).astype(int)\n",
    "\n",
    "test_baseline_metrics = {\n",
    "    \"f1_micro\": f1_score(test_true, test_pred_baseline, average=\"micro\", zero_division=0),\n",
    "    \"f1_macro\": f1_score(test_true, test_pred_baseline, average=\"macro\", zero_division=0),\n",
    "    \"precision\": precision_score(test_true, test_pred_baseline, average=\"micro\", zero_division=0),\n",
    "    \"recall\": recall_score(test_true, test_pred_baseline, average=\"micro\", zero_division=0),\n",
    "}\n",
    "\n",
    "# Tuned on test (frozen thresholds from validation)\n",
    "test_pred_tuned = np.zeros_like(test_probs, dtype=int)\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    test_pred_tuned[:, i] = (test_probs[:, i] >= optimal_thresholds[topic]).astype(int)\n",
    "\n",
    "test_tuned_metrics = {\n",
    "    \"f1_micro\": f1_score(test_true, test_pred_tuned, average=\"micro\", zero_division=0),\n",
    "    \"f1_macro\": f1_score(test_true, test_pred_tuned, average=\"macro\", zero_division=0),\n",
    "    \"precision\": precision_score(test_true, test_pred_tuned, average=\"micro\", zero_division=0),\n",
    "    \"recall\": recall_score(test_true, test_pred_tuned, average=\"micro\", zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"Test set — Baseline (0.5) vs Tuned (frozen from validation):\")\n",
    "print(f\"{'metric':>12s}  {'baseline':>8s}  {'tuned':>8s}  {'delta':>8s}\")\n",
    "print(f\"{'—'*12}  {'—'*8}  {'—'*8}  {'—'*8}\")\n",
    "for k in [\"f1_micro\", \"f1_macro\", \"precision\", \"recall\"]:\n",
    "    b = test_baseline_metrics[k]\n",
    "    t = test_tuned_metrics[k]\n",
    "    print(f\"{k:>12s}  {b:8.4f}  {t:8.4f}  {t - b:+8.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"  Baseline macro F1 (test): {test_baseline_metrics['f1_macro']:.4f}\")\n",
    "print(f\"  Tuned macro F1 (test):    {test_tuned_metrics['f1_macro']:.4f}\")\n",
    "improvement = test_tuned_metrics[\"f1_macro\"] - test_baseline_metrics[\"f1_macro\"]\n",
    "print(f\"  Improvement:              {improvement:+.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Thresholds to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"news_classifier_thresholds.json\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(optimal_thresholds, f, indent=2)\n",
    "\n",
    "print(f\"Thresholds saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. F1 vs Threshold Curves (3 Worst Performing Labels)\n",
    "\n",
    "Curves are from the validation sweep (where tuning happened)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_f1 = sorted(val_baseline_per_class.items(), key=lambda x: x[1])\n",
    "worst_3 = [topic for topic, _ in sorted_by_f1[:3]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, topic in zip(axes, worst_3):\n",
    "    curve = f1_curves[topic]\n",
    "    ts = [t for t, _ in curve]\n",
    "    f1s = [f for _, f in curve]\n",
    "    best_t = optimal_thresholds[topic]\n",
    "    best_f1 = max(f1s)\n",
    "\n",
    "    ax.plot(ts, f1s, linewidth=1.5)\n",
    "    ax.axvline(x=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"default (0.5)\")\n",
    "    ax.axvline(x=best_t, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"optimal ({best_t:.2f})\")\n",
    "    ax.scatter([best_t], [best_f1], color=\"red\", zorder=5)\n",
    "    ax.set_title(topic)\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"F1\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"F1 vs Threshold - 3 Worst Performing Labels (validation)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
