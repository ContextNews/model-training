{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune DistilBERT for News Topic Classification\n",
    "\n",
    "Multi-label classification using `ContextNews/labelled_articles`.\n",
    "\n",
    "**Make sure to set Runtime > Change runtime type > T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "DATASET_ID = \"ContextNews/labelled_articles\"\n",
    "BASE_MODEL = \"distilbert-base-uncased\"\n",
    "PUSH_TO = \"ContextNews/news-classifier\"  # change this to your repo\n",
    "\n",
    "TOPICS = [\n",
    "    \"politics\", \"geopolitics\", \"conflict\", \"crime\", \"law\", \"business\",\n",
    "    \"economy\", \"markets\", \"technology\", \"science\", \"health\", \"environment\",\n",
    "    \"society\", \"education\", \"sports\", \"entertainment\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_ds = load_dataset(DATASET_ID, split=\"train\")\n",
    "val_ds = load_dataset(DATASET_ID, split=\"validation\")\n",
    "test_ds = load_dataset(DATASET_ID, split=\"test\")\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "def build_input_text(row):\n",
    "    title = row.get(\"title\") or \"\"\n",
    "    summary = row.get(\"summary\") or \"\"\n",
    "    text = row.get(\"text\") or \"\"\n",
    "    text_excerpt = \" \".join(text.split()[:300])\n",
    "    return \" \".join(p for p in [title, summary, text_excerpt] if p)\n",
    "\n",
    "\n",
    "def preprocess(row):\n",
    "    row[\"input_text\"] = build_input_text(row)\n",
    "    row[\"labels\"] = [float(row[t] or 0) for t in TOPICS]\n",
    "    return row\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)\n",
    "test_ds = test_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    encoding = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    encoding[\"labels\"] = batch[\"labels\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "cols = train_ds.column_names\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=cols)\n",
    "val_ds = val_ds.map(tokenize, batched=True, remove_columns=cols)\n",
    "test_ds = test_ds.map(tokenize, batched=True, remove_columns=cols)\n",
    "\n",
    "train_ds.set_format(\"torch\")\n",
    "val_ds.set_format(\"torch\")\n",
    "test_ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=len(TOPICS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={i: t for i, t in enumerate(TOPICS)},\n",
    "    label2id={t: i for i, t in enumerate(TOPICS)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    return {\n",
    "        \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"precision\": precision_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=PUSH_TO,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Test set evaluation:\")\n",
    "metrics = trainer.evaluate(test_ds)\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(PUSH_TO)\n",
    "print(f\"Model pushed to {PUSH_TO}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
